{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# From https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html\nimport torch\nimport torchaudio\nimport matplotlib.pyplot as plt\nimport random\nimport math","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# Import data from TIMIT dataset\nTEST_DATA = pd.read_csv(\"/kaggle/input/darpa-timit-acousticphonetic-continuous-speech/test_data.csv\")\nTRAIN_DATA = pd.read_csv(\"/kaggle/input/darpa-timit-acousticphonetic-continuous-speech/train_data.csv\")\nDATA_DIR = \"/kaggle/input/darpa-timit-acousticphonetic-continuous-speech/data/\"\nprint(TRAIN_DATA.path_from_data_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print statistics.\nprint(\"Number of training examples = \" + str(TRAIN_DATA.shape[0]))\nprint(\"Number of test examples = \" + str(TEST_DATA.shape[0]))\nprint(\"Training data shape: \" + str(TRAIN_DATA.shape)) # Should be (train_size, 64, 64, 3).","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert audio to spectrogram using Short Time Fourier Transform (STFT)\ndef load_dataset_as_spectrograms_small(num_audio_samples_per_class_train=10, num_audio_samples_test=500):\n    \"\"\"Loads training and test datasets, from TIMIT and convert into spectrogram using STFT\n\n    Arguments:\n        num_audio_samples_per_class_train: number of audio per class to load into training dataset.\n        num_audio_samples_test: total number of audio samples to load into training dataset.\n    \"\"\"\n    X_train = []\n    X_test = []\n    SAMPLES = 10\n\n    # Create training set\n    TRAIN_WAV = []\n    for audio_sample in TRAIN_DATA.path_from_data_dir:\n        if len(TRAIN_WAV) == SAMPLES:\n            break\n        if type(audio_sample) == str and \"WAV.wav\" in audio_sample:\n            TRAIN_WAV.append(audio_sample)\n    random.shuffle(TRAIN_WAV)\n\n    for audio_sample_path in TRAIN_WAV:\n        waveform, sample_rate = torchaudio.load_wav(DATA_DIR + audio_sample_path)\n        specgram = torchaudio.transforms.MelSpectrogram(n_fft=512, win_length=10)(waveform)\n        X_train.append(specgram)\n\n    TEST_WAV = []\n    for audio_sample in TEST_DATA.path_from_data_dir:\n        if len(TEST_WAV) == SAMPLES:\n            break\n        if type(audio_sample) == str and \"WAV.wav\" in audio_sample:\n            TEST_WAV.append(audio_sample)\n    random.shuffle(TEST_WAV)\n\n    for audio_sample_path in TEST_WAV:\n        waveform, sample_rate = torchaudio.load_wav(DATA_DIR + audio_sample_path)\n        specgram = torchaudio.transforms.MelSpectrogram(n_fft=512, win_length=10)(waveform)\n        X_test.append(specgram)\n\n    # padding\n    max_len = 0\n    for specgram in X_train + X_test:\n        if len(specgram[0][0]) > max_len:\n            max_len = len(specgram[0][0])\n    X_train_pad = []\n    X_test_pad = []\n    for specgram in X_train:\n        pad_by = max_len - len(specgram[0][0])\n        specgram_pad = F.pad(specgram, (0, pad_by, 0, 0), mode='constant')\n        X_train_pad.append(specgram_pad.numpy())\n    for specgram in X_test:\n        pad_by = max_len - len(specgram[0][0])\n        specgram_pad = F.pad(specgram, (0, pad_by, 0, 0), mode='constant')\n        X_test_pad.append(specgram_pad.numpy())\n\n    # Return train and test data as numpy arrays.\n    return np.array(X_train_pad), np.array(X_test_pad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load dataset\nTRAINING_DATASET, TEST_DATASET = load_dataset_as_spectrograms_small()\nprint(TRAINING_DATASET.shape)\nprint(TEST_DATASET.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We split training set into two halfs.\n# First half is used for training as secret images, second half for cover images.\n\n# S: secret image\ninput_S = TRAINING_DATASET[0:X_train.shape[0] // 2]\n\n# C: cover image\ninput_C = TEST_DATASET[X_train.shape[0] // 2:]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show sample spectrograms from the training dataset\nfig=plt.figure(figsize=(8, 8))\ncolumns = 4\nrows = 5\nfor i in range(1, columns*rows +1):\n    # Randomly sample from training dataset\n    img_idx = np.random.choice(TRAINING_DATASET)\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(TRAINING_DATASET[img_idx])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}